{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOx8m2WvSJxyct68vF3D+In",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onseventhflow/house-price-prediction-regression/blob/main/house_price_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB0gZlNcbeye",
        "outputId": "940eaa53-b83b-4123-f8dd-83537358786f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j_SQK64YQ1ML"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict house prices based on 10 numerical features\n",
        "# Generate synthetic dataset (1000 samples, 10 features)\n",
        "np.random.seed(42)\n",
        "\n",
        "x_train = np.random.rand(1000, 10)\n",
        "y_train = np.random.rand(1000) * 500000   # House prices between 0-500k\n",
        "\n",
        "x_test = np.random.rand(200, 10)\n",
        "y_test = np.random.rand(200) * 500000\n",
        "\n",
        "# Build model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(10,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)   # Single output for regression\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_mae = model.evaluate(x_test, y_test)\n",
        "print(f\"Test MAE: ${test_mae:.2f}\")\n",
        "\n",
        "# Make a prediction\n",
        "sample_input = np.random.rand(1, 10)\n",
        "predicted_price = model.predict(sample_input)\n",
        "print(f\"Predicted House Price: ${predicted_price[0][0]:.2f}\")\n",
        "\n",
        "# MAE is around 243K - 245K, which means on average, the predictions are off by nearly half of the actual house price.\n",
        "# This is very bad because an error of $243K in a price range of $500K means the model is almost random.\n",
        "# The dataset is completely random, meaning there is no real pattern for the model to learn.\n",
        "# The model is guessing house prices randomly, which is why MAE is so large.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUpHGQlkQ9di",
        "outputId": "c565bd2e-eb79-4c3f-81e1-12fc0b0784af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 82080382976.0000 - mae: 248890.8281\n",
            "Epoch 2/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 83513565184.0000 - mae: 251576.9062\n",
            "Epoch 3/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84896489472.0000 - mae: 252251.7500\n",
            "Epoch 4/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 86818447360.0000 - mae: 257159.5625\n",
            "Epoch 5/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 83349413888.0000 - mae: 251876.6875\n",
            "Epoch 6/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 84547911680.0000 - mae: 252302.8125\n",
            "Epoch 7/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 82303762432.0000 - mae: 247729.8438\n",
            "Epoch 8/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 85053333504.0000 - mae: 254944.3438\n",
            "Epoch 9/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 80920903680.0000 - mae: 244842.0156\n",
            "Epoch 10/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84122796032.0000 - mae: 251746.5312\n",
            "Epoch 11/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85094055936.0000 - mae: 254165.2031\n",
            "Epoch 12/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86400114688.0000 - mae: 256451.1250\n",
            "Epoch 13/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 86340558848.0000 - mae: 255346.0625\n",
            "Epoch 14/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80998899712.0000 - mae: 246393.4844\n",
            "Epoch 15/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85025521664.0000 - mae: 254443.1406\n",
            "Epoch 16/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 78787641344.0000 - mae: 241923.2344\n",
            "Epoch 17/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82137243648.0000 - mae: 247950.8594\n",
            "Epoch 18/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82727190528.0000 - mae: 248413.5469\n",
            "Epoch 19/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80438624256.0000 - mae: 243856.3125\n",
            "Epoch 20/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 81621991424.0000 - mae: 247291.2500\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 83073081344.0000 - mae: 243433.8438\n",
            "Test MAE: $234842.14\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463ms/step\n",
            "Predicted House Price: $11123.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - House Price Prediction (Regression) with Synthetic dataset and Normalization\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic dataset (1000 samples, 10 features) in [0,1] range\n",
        "np.random.seed(42)\n",
        "\n",
        "x_train = np.random.rand(1000, 10)\n",
        "y_train = np.random.rand(1000)   # Target values in range 0-1\n",
        "\n",
        "x_test = np.random.rand(200, 10)\n",
        "y_test = np.random.rand(200)     # Targets in range 0-1\n",
        "\n",
        "# Build model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(10,)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)   # Single output for regression\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train model\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_mae = model.evaluate(x_test, y_test)\n",
        "print(f\"Test MAE (Normalized): {test_mae:.4f}\")\n",
        "\n",
        "# Make a prediction\n",
        "sample_input = np.random.rand(1, 10)   # In [0,1] range\n",
        "predicted_value = model.predict(sample_input)\n",
        "print(f\"Predicted Value (Normalized): {predicted_value[0][0]:.4f}\")\n",
        "\n",
        "# This is again a bad model because we are still training on random values (just normalized values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vv42QAFR2w1",
        "outputId": "ac9c8780-407c-4caa-dd28-f179c7b8fb60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 0.1262 - mae: 0.2917\n",
            "Epoch 2/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0880 - mae: 0.2536\n",
            "Epoch 3/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0836 - mae: 0.2452\n",
            "Epoch 4/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0861 - mae: 0.2491\n",
            "Epoch 5/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0779 - mae: 0.2394\n",
            "Epoch 6/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0789 - mae: 0.2389\n",
            "Epoch 7/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0793 - mae: 0.2404\n",
            "Epoch 8/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0811 - mae: 0.2442\n",
            "Epoch 9/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0809 - mae: 0.2430\n",
            "Epoch 10/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0809 - mae: 0.2423\n",
            "Epoch 11/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0763 - mae: 0.2366\n",
            "Epoch 12/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0792 - mae: 0.2409\n",
            "Epoch 13/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0781 - mae: 0.2391\n",
            "Epoch 14/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0790 - mae: 0.2421\n",
            "Epoch 15/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0760 - mae: 0.2359\n",
            "Epoch 16/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0755 - mae: 0.2331\n",
            "Epoch 17/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0735 - mae: 0.2303\n",
            "Epoch 18/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0744 - mae: 0.2341\n",
            "Epoch 19/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0767 - mae: 0.2386\n",
            "Epoch 20/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0759 - mae: 0.2367\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.1013 - mae: 0.2734\n",
            "Test MAE (Normalized): 0.2682\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
            "Predicted Value (Normalized): 0.5643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - House Price Prediction (Regression) using California Housing Dataset SCIKIT LEARN Dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load California Housing dataset (CSV version – works in Colab)\n",
        "df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\"\n",
        ")\n",
        "\n",
        "# Use only numerical features (drop categorical column)\n",
        "x = df.drop([\"median_house_value\", \"ocean_proximity\"], axis=1).values\n",
        "y = df[\"median_house_value\"].values / 100000   # scale to 100,000s\n",
        "\n",
        "print(\n",
        "    f\"Target Value Range (in 100,000s): \"\n",
        "    f\"Min = {y.min():.2f}, Max = {y.max():.2f}, Mean = {y.mean():.2f}\"\n",
        ")\n",
        "\n",
        "# Split data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Normalize features – Z-Score Standardization\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "# Build neural network model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)   # Single output for house price\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train model\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_mae = model.evaluate(x_test, y_test)\n",
        "print(f\"Test MAE: ${test_mae * 100000:.2f}\")\n",
        "\n",
        "# Make a prediction\n",
        "sample_input = np.expand_dims(x_test[0], axis=0)\n",
        "predicted_price = model.predict(sample_input)\n",
        "print(f\"Predicted House Price: ${predicted_price[0][0] * 100000:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEAFcG4kWOhT",
        "outputId": "f1b01d18-17c7-46de-8031-bade557694cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Value Range (in 100,000s): Min = 0.15, Max = 5.00, Mean = 2.07\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.1265 - mae: 1.0002\n",
            "Epoch 2/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4004 - mae: 0.4515\n",
            "Epoch 3/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3661 - mae: 0.4332\n",
            "Epoch 4/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3612 - mae: 0.4288\n",
            "Epoch 5/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3525 - mae: 0.4192\n",
            "Epoch 6/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3259 - mae: 0.4023\n",
            "Epoch 7/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3194 - mae: 0.4006\n",
            "Epoch 8/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3065 - mae: 0.3905\n",
            "Epoch 9/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3103 - mae: 0.3870\n",
            "Epoch 10/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3012 - mae: 0.3843\n",
            "Epoch 11/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3113 - mae: 0.3872\n",
            "Epoch 12/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3036 - mae: 0.3844\n",
            "Epoch 13/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2959 - mae: 0.3770\n",
            "Epoch 14/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3018 - mae: 0.3768\n",
            "Epoch 15/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2946 - mae: 0.3745\n",
            "Epoch 16/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2876 - mae: 0.3684\n",
            "Epoch 17/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2972 - mae: 0.3760\n",
            "Epoch 18/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2800 - mae: 0.3666\n",
            "Epoch 19/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2929 - mae: 0.3731\n",
            "Epoch 20/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2829 - mae: 0.3674\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - mae: nan\n",
            "Test MAE: $nan\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
            "Predicted House Price: $nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #  - House Price Prediction (Regression) with SCIKIT Dataset\n",
        "\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers, models\n",
        "# from sklearn.datasets import fetch_california_housing\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# import numpy as np\n",
        "\n",
        "# # Load California housing dataset\n",
        "# data = fetch_california_housing()\n",
        "# x, y = data.data, data.target   # Features and target (median house value in 100,000s)\n",
        "\n",
        "# print(\n",
        "#     f\"Target Value Range (in 100,000s): \"\n",
        "#     f\"Min = {y.min():.2f}, Max = {y.max():.2f}, Mean = {y.mean():.2f}\"\n",
        "# )\n",
        "\n",
        "# # Split data into train and test sets\n",
        "# x_train, x_test, y_train, y_test = train_test_split(\n",
        "#     x, y, test_size=0.2, random_state=42\n",
        "# )\n",
        "\n",
        "# # Normalize features for better training stability – Z-Score Standardization / Standard scaling\n",
        "# scaler = StandardScaler()\n",
        "# x_train = scaler.fit_transform(x_train)   # Compute mean & std from training data, then scale\n",
        "# x_test = scaler.transform(x_test)          # Use the same scaling parameters to transform test data\n",
        "\n",
        "# # We use the same mean and standard deviation (computed from x_train) to scale x_test\n",
        "# # This ensures that both training and test data follow the same distribution\n",
        "\n",
        "# # If we include x_test when computing mean and standard deviation, the model gets information from the test set before training\n",
        "# # This is called data leakage, which can lead to unrealistically good performance and poor generalization to new data\n",
        "# # Test data should only be used for evaluation after the model is trained\n",
        "\n",
        "# # In real-world applications, new data arrives after training. We do not get to recompute the mean and std for each new data point\n",
        "# # The model should be able to handle unseen data using the same scaling applied during training\n",
        "\n",
        "\n",
        "# # Build model\n",
        "# model = models.Sequential([\n",
        "#     layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),  # Input features from dataset\n",
        "#     layers.Dense(32, activation='relu'),\n",
        "#     layers.Dense(1)   # Single output for regression\n",
        "# ])\n",
        "\n",
        "# # Compile model\n",
        "# model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# # Train model\n",
        "# model.fit(x_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
        "\n",
        "# # Evaluate model\n",
        "# test_loss, test_mae = model.evaluate(x_test, y_test)\n",
        "# print(f\"Test MAE: ${test_mae * 100000:.2f}\")   # Convert to actual dollars\n",
        "\n",
        "# # Make a prediction\n",
        "# sample_input = np.expand_dims(x_test[0], axis=0)   # Take one test sample\n",
        "# predicted_price = model.predict(sample_input)\n",
        "# print(f\"Predicted House Price: ${predicted_price[0][0] * 100000:.2f}\")  # Convert to actual dollars\n"
      ],
      "metadata": {
        "id": "tCVvPkcbYggm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}